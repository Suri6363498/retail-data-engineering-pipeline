# -*- coding: utf-8 -*-
"""Retail_Pipeline_Transformations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pQHxTKxMkHJTWx6OATZAWu3hq5kLj2CR
"""

# Install PySpark
!pip install pyspark

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("RetailPipeline").getOrCreate()

# Set file paths (adjust based on your Drive)
base_path = "/content/drive/MyDrive/Retail-Data-Pipeline/"

sales_df = spark.read.option("header", "true").csv(base_path + "sales.csv", inferSchema=True)
customers_df = spark.read.option("header", "true").csv(base_path + "customers.csv", inferSchema=True)
products_df = spark.read.option("header", "true").csv(base_path + "products.csv", inferSchema=True)

# Show data
sales_df.show()
customers_df.show()
products_df.show()

from pyspark.sql.functions import col, expr

# Join dataframes
sales_with_customers = sales_df.join(customers_df, on="customer_id", how="left")
full_data = sales_with_customers.join(products_df, on="product_id", how="left")

# Add a calculated column: total_amount = quantity * price
full_data = full_data.withColumn("total_amount", col("quantity") * col("price"))

# Show final result
full_data.select("sale_id", "name", "region", "product_name", "category", "quantity", "price", "total_amount", "sale_date").show()

# Save as CSV
output_path = base_path + "processed_sales.csv"
full_data.write.option("header", "true").mode("overwrite").csv(output_path)